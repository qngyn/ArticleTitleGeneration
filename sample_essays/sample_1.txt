One of the biggest challenges that is often overlooked for people in academia, especially undergraduate students who are new to writing research papers, is how to write a captivating summary, or introduction, that captures all the details that they have diligently carried out through their research. Thus, I want to develop a model that can generate summary of research or academic papers and essay. I do not expect the outcome model to generate a summary that can be used directly in a paper, but at least it can be something that the writer can rely on and produce a well-crafted introduction more easily.
Since I want to focus on the task of summarizing research and academic papers, I plan to use the data set from ARXIV1, which is available on Kaggle. This dataset provides a JSON object of papers, including the link to the corresponding PDF files. So I expect to be using the PyPDF2 library to extract the text from the PDF file.
There are two forms of text summarization, extractive and abstractive. Extractive summarization works by assigning and ranking weights capturing the importance for phrases or sentences from the source texts, then join them together to form a summary. This method can be implemented using the TextRank algorithm, or more simply without any deep learning method, using the NLTK library to tokenize words and assign weights based on their frequency. On the other hand, abstractive summarization understanding the semantics of the source texts to generate a summary that encapsulate the integral information from the given text. To implement this method, one can utilize deep learning methods and implement the recurrent sequence-to-sequence model using Long Short Term Memory (LSTM) modules with the encoder-decoder architecture to remember the previous parts or the input, in other words, to capture the semantics of the text. According to Russell and Norvig, a basic sequence-to-sequence model has the limitations including nearby context bias (thus cannot capture context that is further away, which is an important aspect for summary) and fixed context size limit (so it only words for shorter input sequences). A solution for this, which I hope to implement, is to use an attentional sequence-to-sequence model, which only look at specific parts of the input sequence. This is reasonable as one does not need an entire sequence to capture its meaning. 
